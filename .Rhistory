tmpl_var_param = foreach(r = seq_len(nrow(tmp_var_param)), .combine = 'rbind',.options.RNG=1234,
.packages=c("dplyr","groundhog","foreach","doParallel")) %dorng% {
source("d:/users/srhuet/documents/BiodivLabel/R/BVIAS_functions.R")
tmp_c = foreach(c = colnames(dplyr::select_if(tmp_var_param,is.numeric)),  # Ignorer les colonnes non-paramètres
.combine='cbind') %dopar% {
tmp_c <- tmp_var_param[r,c] - learning_rate * tmp_gradient
tmp_c
}
# test if new parameters give 0<y<1; else keep unchanged parameters
tmp_BV = tmp_c %>% mutate(y = gamma + epsilon * exp(-(abs(((0.5^delta) - beta) ^ alpha) / (2*sigma^alpha))))
if(!(is.na(tmp_BV$y)) & tmp_BV$y>=0 & tmp_BV$y<=1) {
tmpl_var_param <- tmp_var_param[r,colnames(dplyr::select_if(tmp_var_param,is.character))] %>% cbind(.,tmp_c)
} else {
tmpl_var_param <- tmp_var_param[r,]
}
tmpl_var_param
}
# test if new parameters reduce MSE; else keep unchanged parameters
MSE_old <- BVIAS_loss_function_LU(input, id_cols, tmp_var_param,var_weight)
MSE_new <- BVIAS_loss_function_LU(input, id_cols, tmpl_var_param,var_weight)
if (!is.na(MSE_new$MSE) & MSE_new$MSE <= MSE_old$MSE - tolerance) {
tmp_var_param <- tmpl_var_param
var_param_optim <- tmp_var_param %>% mutate(MSE = MSE_new$MSE)
} else {
#var_param_optim <- tmp_var_param %>% mutate(MSE = MSE_old$MSE)
print(paste0("Model converged at epoch ",epoch))
#break
}
var_param_optim
}
var_param_optim <- var_param_optim %>% filter(MSE == min(MSE))
stopCluster(cl)
return(var_param_optim)
}
library('groundhog')
library('foreach')
library('doParallel')
library('doRNG')
## Parameter optimization ----
tmp_param_BV_constant_optim <- sgd_param(tmp,c("farm_id","crop","land_use_type"),
tmp_param_BV_constant,tmp_param_var_weight,
learning_rate = 0.1, epochs = 500, test_sample_size = 50,tolerance = 1e-5)
library('groundhog')
## Parameter optimization ----
tmp_param_BV_constant_optim <- sgd_param(tmp,c("farm_id","crop","land_use_type"),
tmp_param_BV_constant,tmp_param_var_weight,
learning_rate = 0.1, epochs = 500, test_sample_size = 50,tolerance = 1e-5)
var_param_optim = foreach(epoch = 1:epochs,.combine = 'rbind',.options.RNG=1234,
.packages=c("dplyr","groundhog","foreach","doParallel")) %dorng% {
#for (epoch in 1:epochs) {
#library(dplyr)
library('groundhog')
#library('foreach')
#library('doParallel')
source("d:/users/srhuet/documents/BiodivLabel/R/BVIAS_functions.R")
# Randomly sample some data point
index <- sample(1:nrow(input), test_sample_size)
input_i <- input[index,]
# Compute the gradient of the loss function
## the gradient is obtained by calculating the partial derivative of the loss function
tmp_new_cost <-  BVIAS_loss_function_LU(input_i, id_cols, tmp_var_param,var_weight)
## WIP I think I should compute one gradient per parameter based on the derivative of the loss function according to this parameter
tmp_gradient <- -2/test_sample_size * sum(tmp_new_cost$distance_table$litt_values - tmp_new_cost$distance_table$BVAS)
# Update model parameters
tmpl_var_param = foreach(r = seq_len(nrow(tmp_var_param)), .combine = 'rbind',.options.RNG=1234,
.packages=c("dplyr","groundhog","foreach","doParallel")) %dorng% {
library('groundhog')
source("d:/users/srhuet/documents/BiodivLabel/R/BVIAS_functions.R")
tmp_c = foreach(c = colnames(dplyr::select_if(tmp_var_param,is.numeric)),  # Ignorer les colonnes non-paramètres
.combine='cbind') %dopar% {
tmp_c <- tmp_var_param[r,c] - learning_rate * tmp_gradient
tmp_c
}
# test if new parameters give 0<y<1; else keep unchanged parameters
tmp_BV = tmp_c %>% mutate(y = gamma + epsilon * exp(-(abs(((0.5^delta) - beta) ^ alpha) / (2*sigma^alpha))))
if(!(is.na(tmp_BV$y)) & tmp_BV$y>=0 & tmp_BV$y<=1) {
tmpl_var_param <- tmp_var_param[r,colnames(dplyr::select_if(tmp_var_param,is.character))] %>% cbind(.,tmp_c)
} else {
tmpl_var_param <- tmp_var_param[r,]
}
tmpl_var_param
}
# test if new parameters reduce MSE; else keep unchanged parameters
MSE_old <- BVIAS_loss_function_LU(input, id_cols, tmp_var_param,var_weight)
MSE_new <- BVIAS_loss_function_LU(input, id_cols, tmpl_var_param,var_weight)
if (!is.na(MSE_new$MSE) & MSE_new$MSE <= MSE_old$MSE - tolerance) {
tmp_var_param <- tmpl_var_param
var_param_optim <- tmp_var_param %>% mutate(MSE = MSE_new$MSE)
} else {
#var_param_optim <- tmp_var_param %>% mutate(MSE = MSE_old$MSE)
print(paste0("Model converged at epoch ",epoch))
#break
}
var_param_optim
}
sgd_param <- function(input,id_cols,var_param,var_weight,learning_rate, epochs,test_sample_size,tolerance) {
library('groundhog')
library('foreach')
library('doParallel')
library('doRNG')
#get the core processors ready to go
cl <- round(parallel::detectCores()*0.8)
registerDoParallel(cl) #I leave 20% free to do things while R runs
# test
#input = tmp
#id_cols = c("farm_id","crop","land_use_type")
#var_param = tmp_param_BV_constant
#var_weight = tmp_param_var_weight
#learning_rate = 0.01
#epochs = 10 # nb iterations
#test_sample_size = 10 # nb of randomly selected obs per iteration
#tolerance = 1e-5 # relative convergence tolerance
tmp_base_cost <- BVIAS_loss_function_LU(input, id_cols, var_param,var_weight)
tmp_var_param = var_param
var_param_optim = foreach(epoch = 1:epochs,.combine = 'rbind',.options.RNG=1234,
.packages=c("dplyr","groundhog","foreach","doParallel")) %dorng% {
#for (epoch in 1:epochs) {
#library(dplyr)
library('groundhog')
#library('foreach')
#library('doParallel')
source("d:/users/srhuet/documents/BiodivLabel/R/BVIAS_functions.R")
# Randomly sample some data point
index <- sample(1:nrow(input), test_sample_size)
input_i <- input[index,]
# Compute the gradient of the loss function
## the gradient is obtained by calculating the partial derivative of the loss function
tmp_new_cost <-  BVIAS_loss_function_LU(input_i, id_cols, tmp_var_param,var_weight)
## WIP I think I should compute one gradient per parameter based on the derivative of the loss function according to this parameter
tmp_gradient <- -2/test_sample_size * sum(tmp_new_cost$distance_table$litt_values - tmp_new_cost$distance_table$BVAS)
# Update model parameters
tmpl_var_param = foreach(r = seq_len(nrow(tmp_var_param)), .combine = 'rbind',.options.RNG=1234,
.packages=c("dplyr","groundhog","foreach","doParallel")) %dorng% {
library('groundhog')
source("d:/users/srhuet/documents/BiodivLabel/R/BVIAS_functions.R")
tmp_c = foreach(c = colnames(dplyr::select_if(tmp_var_param,is.numeric)),  # Ignorer les colonnes non-paramètres
.combine='cbind') %dopar% {
tmp_c <- tmp_var_param[r,c] - learning_rate * tmp_gradient
tmp_c
}
# test if new parameters give 0<y<1; else keep unchanged parameters
tmp_BV = tmp_c %>% mutate(y = gamma + epsilon * exp(-(abs(((0.5^delta) - beta) ^ alpha) / (2*sigma^alpha))))
if(!(is.na(tmp_BV$y)) & tmp_BV$y>=0 & tmp_BV$y<=1) {
tmpl_var_param <- tmp_var_param[r,colnames(dplyr::select_if(tmp_var_param,is.character))] %>% cbind(.,tmp_c)
} else {
tmpl_var_param <- tmp_var_param[r,]
}
tmpl_var_param
}
# test if new parameters reduce MSE; else keep unchanged parameters
MSE_old <- BVIAS_loss_function_LU(input, id_cols, tmp_var_param,var_weight)
MSE_new <- BVIAS_loss_function_LU(input, id_cols, tmpl_var_param,var_weight)
if (!is.na(MSE_new$MSE) & MSE_new$MSE <= MSE_old$MSE - tolerance) {
tmp_var_param <- tmpl_var_param
var_param_optim <- tmp_var_param %>% mutate(MSE = MSE_new$MSE)
} else {
#var_param_optim <- tmp_var_param %>% mutate(MSE = MSE_old$MSE)
print(paste0("Model converged at epoch ",epoch))
#break
}
var_param_optim
}
var_param_optim <- var_param_optim %>% filter(MSE == min(MSE))
stopCluster(cl)
return(var_param_optim)
}
## Parameter optimization ----
tmp_param_BV_constant_optim <- sgd_param(tmp,c("farm_id","crop","land_use_type"),
tmp_param_BV_constant,tmp_param_var_weight,
learning_rate = 0.1, epochs = 500, test_sample_size = 50,tolerance = 1e-5)
#get the core processors ready to go
cl <- round(parallel::detectCores()*0.8)
registerDoParallel(cl) #I leave 20% free to do things while R runs
input = tmp
id_cols = c("farm_id","crop","land_use_type")
var_param = tmp_param_BV_constant
var_weight = tmp_param_var_weight
#input = tmp
#id_cols = c("farm_id","crop","land_use_type")
#var_param = tmp_param_BV_constant
#var_weight = tmp_param_var_weight
#learning_rate = 0.01
#epochs = 10 # nb iterations
#test_sample_size = 10 # nb of randomly selected obs per iteration
#tolerance = 1e-5 # relative convergence tolerance
input = tmp
id_cols = c("farm_id","crop","land_use_type")
var_param = tmp_param_BV_constant
var_weight = tmp_param_var_weight
learning_rate = 0.01
epochs = 10 # nb iterations
test_sample_size = 10 # nb of randomly selected obs per iteration
tolerance = 1e-5 # relative convergence tolerance
tmp_base_cost <- BVIAS_loss_function_LU(input, id_cols, var_param,var_weight)
tmp_var_param = var_param
var_param_optim = foreach(epoch = 1:epochs,.combine = 'rbind',.options.RNG=1234,
.packages=c("dplyr","groundhog","foreach","doParallel")) %dorng% {
#for (epoch in 1:epochs) {
#library(dplyr)
library('groundhog')
#library('foreach')
#library('doParallel')
source("d:/users/srhuet/documents/BiodivLabel/R/BVIAS_functions.R")
# Randomly sample some data point
index <- sample(1:nrow(input), test_sample_size)
input_i <- input[index,]
# Compute the gradient of the loss function
## the gradient is obtained by calculating the partial derivative of the loss function
tmp_new_cost <-  BVIAS_loss_function_LU(input_i, id_cols, tmp_var_param,var_weight)
## WIP I think I should compute one gradient per parameter based on the derivative of the loss function according to this parameter
tmp_gradient <- -2/test_sample_size * sum(tmp_new_cost$distance_table$litt_values - tmp_new_cost$distance_table$BVAS)
# Update model parameters
tmpl_var_param = foreach(r = seq_len(nrow(tmp_var_param)), .combine = 'rbind',.options.RNG=1234,
.packages=c("dplyr","groundhog","foreach","doParallel")) %dorng% {
library('groundhog')
source("d:/users/srhuet/documents/BiodivLabel/R/BVIAS_functions.R")
tmp_c = foreach(c = colnames(dplyr::select_if(tmp_var_param,is.numeric)),  # Ignorer les colonnes non-paramètres
.combine='cbind') %dopar% {
tmp_c <- tmp_var_param[r,c] - learning_rate * tmp_gradient
tmp_c
}
# test if new parameters give 0<y<1; else keep unchanged parameters
tmp_BV = tmp_c %>% mutate(y = gamma + epsilon * exp(-(abs(((0.5^delta) - beta) ^ alpha) / (2*sigma^alpha))))
if(!(is.na(tmp_BV$y)) & tmp_BV$y>=0 & tmp_BV$y<=1) {
tmpl_var_param <- tmp_var_param[r,colnames(dplyr::select_if(tmp_var_param,is.character))] %>% cbind(.,tmp_c)
} else {
tmpl_var_param <- tmp_var_param[r,]
}
tmpl_var_param
}
# test if new parameters reduce MSE; else keep unchanged parameters
MSE_old <- BVIAS_loss_function_LU(input, id_cols, tmp_var_param,var_weight)
MSE_new <- BVIAS_loss_function_LU(input, id_cols, tmpl_var_param,var_weight)
if (!is.na(MSE_new$MSE) & MSE_new$MSE <= MSE_old$MSE - tolerance) {
tmp_var_param <- tmpl_var_param
var_param_optim <- tmp_var_param %>% mutate(MSE = MSE_new$MSE)
} else {
#var_param_optim <- tmp_var_param %>% mutate(MSE = MSE_old$MSE)
print(paste0("Model converged at epoch ",epoch))
#break
}
var_param_optim
}
var_param_optim <- var_param_optim %>% filter(MSE == min(MSE))
stopCluster(cl)
sgd_param <- function(input,id_cols,var_param,var_weight,learning_rate, epochs,test_sample_size,tolerance) {
library('groundhog')
library('foreach')
library('doParallel')
library('doRNG')
#get the core processors ready to go
registerDoParallel(cores = round(parallel::detectCores()*0.8)) #I leave 20% free to do things while R runs
# test
#input = tmp
#id_cols = c("farm_id","crop","land_use_type")
#var_param = tmp_param_BV_constant
#var_weight = tmp_param_var_weight
#learning_rate = 0.01
#epochs = 10 # nb iterations
#test_sample_size = 10 # nb of randomly selected obs per iteration
#tolerance = 1e-5 # relative convergence tolerance
tmp_base_cost <- BVIAS_loss_function_LU(input, id_cols, var_param,var_weight)
tmp_var_param = var_param
var_param_optim = foreach(epoch = 1:epochs,.combine = 'rbind',.options.RNG=1234,
.packages=c("dplyr","groundhog","foreach","doParallel")) %dorng% {
#for (epoch in 1:epochs) {
#library(dplyr)
library('groundhog')
#library('foreach')
#library('doParallel')
source("d:/users/srhuet/documents/BiodivLabel/R/BVIAS_functions.R")
# Randomly sample some data point
index <- sample(1:nrow(input), test_sample_size)
input_i <- input[index,]
# Compute the gradient of the loss function
## the gradient is obtained by calculating the partial derivative of the loss function
tmp_new_cost <-  BVIAS_loss_function_LU(input_i, id_cols, tmp_var_param,var_weight)
## WIP I think I should compute one gradient per parameter based on the derivative of the loss function according to this parameter
tmp_gradient <- -2/test_sample_size * sum(tmp_new_cost$distance_table$litt_values - tmp_new_cost$distance_table$BVAS)
# Update model parameters
tmpl_var_param = foreach(r = seq_len(nrow(tmp_var_param)), .combine = 'rbind',.options.RNG=1234,
.packages=c("dplyr","groundhog","foreach","doParallel")) %dorng% {
library('groundhog')
source("d:/users/srhuet/documents/BiodivLabel/R/BVIAS_functions.R")
tmp_c = foreach(c = colnames(dplyr::select_if(tmp_var_param,is.numeric)),  # Ignorer les colonnes non-paramètres
.combine='cbind') %dopar% {
tmp_c <- tmp_var_param[r,c] - learning_rate * tmp_gradient
tmp_c
}
# test if new parameters give 0<y<1; else keep unchanged parameters
tmp_BV = tmp_c %>% mutate(y = gamma + epsilon * exp(-(abs(((0.5^delta) - beta) ^ alpha) / (2*sigma^alpha))))
if(!(is.na(tmp_BV$y)) & tmp_BV$y>=0 & tmp_BV$y<=1) {
tmpl_var_param <- tmp_var_param[r,colnames(dplyr::select_if(tmp_var_param,is.character))] %>% cbind(.,tmp_c)
} else {
tmpl_var_param <- tmp_var_param[r,]
}
tmpl_var_param
}
# test if new parameters reduce MSE; else keep unchanged parameters
MSE_old <- BVIAS_loss_function_LU(input, id_cols, tmp_var_param,var_weight)
MSE_new <- BVIAS_loss_function_LU(input, id_cols, tmpl_var_param,var_weight)
if (!is.na(MSE_new$MSE) & MSE_new$MSE <= MSE_old$MSE - tolerance) {
tmp_var_param <- tmpl_var_param
var_param_optim <- tmp_var_param %>% mutate(MSE = MSE_new$MSE)
} else {
#var_param_optim <- tmp_var_param %>% mutate(MSE = MSE_old$MSE)
print(paste0("Model converged at epoch ",epoch))
#break
}
var_param_optim
}
var_param_optim <- var_param_optim %>% filter(MSE == min(MSE))
return(var_param_optim)
}
rm(tmp_param_BV_constant_optim)
source("d:/users/srhuet/documents/BiodivLabel/R/BVIAS_functions.R")
tmp <- head(tmp_input[tmp_input$land_use_type == "arable" & tmp_input$org_farming == F,],n = 70) %>%
rbind(.,head(tmp_input[tmp_input$land_use_type == "grassland" & tmp_input$org_farming == F,],n = 30)) %>%
rbind(.,head(tmp_input[tmp_input$land_use_type == "arable" & tmp_input$org_farming == T,],n = 70)) %>%
rbind(.,head(tmp_input[tmp_input$land_use_type == "grassland" & tmp_input$org_farming == T,],n = 30))
tmp_x_norm <- data_for_BVIAS(tmp,c("farm_id","crop","land_use_type"),tmp_param_BV_constant)
tmp_BVIAS <- BVIAS(tmp_x_norm,c("farm_id","crop","land_use_type"),tmp_param_BV_constant,tmp_param_var_weight,T)
library('groundhog')
library('foreach')
library('doParallel')
library('doRNG')
## Parameter optimization ----
tmp_param_BV_constant_optim <- sgd_param(tmp,c("farm_id","crop","land_use_type"),
tmp_param_BV_constant,tmp_param_var_weight,
learning_rate = 0.1, epochs = 500, test_sample_size = 50,tolerance = 1e-5)
View(tmp_param_BV_constant_optim)
tmp_BVIAS_optim1 <- BVIAS(tmp_x_norm,c("farm_id","crop","land_use_type"),tmp_param_BV_constant_optim,tmp_param_var_weight,T)
# compare MSE
tmp_MSE <- BVIAS_loss_function_LU(tmp,c("farm_id","crop","land_use_type"),tmp_param_BV_constant,tmp_param_var_weight)
tmp_MSE_optim <- BVIAS_loss_function_LU(tmp,c("farm_id","crop","land_use_type"),
tmp_param_BV_constant_optim,
tmp_param_var_weight)
library('doRNG')
# test
#input = tmp
#id_cols = c("farm_id","crop","land_use_type","org_farming")
#var_param = tmp_param_BV_constant_optim
#var_weight = tmp_param_var_weight
#crop_name_cereals = unique(na.omit(tmp_TT_crops$crop[tmp_TT_crops$species == "cereal"]))
#learning_rate = 0.01
#epochs = 10
#test_sample_size = 100
# test
#input = tmp
#id_cols = c("farm_id","crop","land_use_type","org_farming")
#var_param = tmp_param_BV_constant_optim
#var_weight = tmp_param_var_weight
#crop_name_cereals = unique(na.omit(tmp_TT_crops$crop[tmp_TT_crops$species == "cereal"]))
#learning_rate = 0.01
#epochs = 10
#test_sample_size = 10 # nb of randomly selected obs per iteration
#tolerance = 1e-5 # relative convergence tolerance
# test
input = tmp
id_cols = c("farm_id","crop","land_use_type","org_farming")
var_param = tmp_param_BV_constant_optim
var_weight = tmp_param_var_weight
crop_name_cereals = unique(na.omit(tmp_TT_crops$crop[tmp_TT_crops$species == "cereal"]))
learning_rate = 0.01
epochs = 10
test_sample_size = 10 # nb of randomly selected obs per iteration
tolerance = 1e-5 # relative convergence tolerance
tmp_base_cost <- BVIAS_loss_function_weight(input,id_cols,var_param,var_weight,crop_name_cereals)
tmp_var_weight = var_weight
# Randomly sample some data point
index <- sample(1:nrow(input), test_sample_size)
input_i <- input[index,]
7*8
## WIP I think I should compute one gradient per parameter based on the partial derivative of the loss function according to this parameter
tmp_gradient <- -2/test_sample_size * sum(tmp_new_cost$distance_table$litt_values - tmp_new_cost$distance_table$BVAS)
# Compute the gradient of the loss function
## the gradient is obtained by calculating the partial derivative of the loss function
tmp_new_cost <-  BVIAS_loss_function_weight(input_i, id_cols,var_param,var_weight,crop_name_cereals)
## WIP I think I should compute one gradient per parameter based on the partial derivative of the loss function according to this parameter
tmp_gradient <- -2/test_sample_size * sum(tmp_new_cost$distance_table$litt_values - tmp_new_cost$distance_table$BVAS)
View(tmp_var_weight)
var_weight = tmp_param_var_weight %>% mutate(weight = 1/length(metric_number))
# Update model parameters
tmpl_var_weight = foreach(r = seq_len(nrow(tmp_var_weight)), .combine = 'rbind',.options.RNG=1234,
.packages=c("dplyr","groundhog","foreach","doParallel")) %dorng% {
source("d:/users/srhuet/documents/BiodivLabel/R/BVIAS_functions.R")
tmpl_var_weight <- tmpl_var_weight %>%
mutate(weight = weight - learning_rate * tmp_gradient)
tmpl_var_weight
}
# Update model parameters
tmpl_var_weight = foreach(r = seq_len(nrow(tmp_var_weight)), .combine = 'rbind',.options.RNG=1234,
.packages=c("dplyr","groundhog","foreach","doParallel")) %dorng% {
source("d:/users/srhuet/documents/BiodivLabel/R/BVIAS_functions.R")
tmpl_var_weight <- tmp_var_weight %>%
mutate(weight = weight - learning_rate * tmp_gradient)
tmpl_var_weight
}
# test if new parameters reduce MSE; else keep unchanged parameters
MSE_old <- BVIAS_loss_function_LU(input, id_cols, var_param, tmp_var_weight)
MSE_new <- BVIAS_loss_function_LU(input, id_cols, var_param, tmpl_var_weight)
if (!is.na(MSE_new$MSE) & MSE_new$MSE < MSE_old$MSE) {
tmp_var_weight <- tmpl_var_weight
var_weight_optim <- tmp_var_weight %>% mutate(MSE = MSE_new$MSE)
} else {
#var_param_optim <- tmp_var_param %>% mutate(MSE = MSE_old$MSE)
print(paste0("Model converged at epoch ",epoch))
#break
}
# test if new parameters reduce MSE; else keep unchanged parameters
MSE_old <- BVIAS_loss_function_weight(input, id_cols, var_param, tmp_var_weight)
MSE_new <- BVIAS_loss_function_weight(input, id_cols, var_param, tmpl_var_weight)
input = tmp
# test if new parameters reduce MSE; else keep unchanged parameters
MSE_old <- BVIAS_loss_function_weight(input, id_cols, var_param, tmp_var_weight)
# test
#input = tmp
#id_cols = c("farm_id","crop","land_use_type","org_farming")
#var_param = tmp_param_BV_constant_optim
#var_weight = tmp_param_var_weight %>% mutate(weight = 1/length(metric_number))
#crop_name_cereals = unique(na.omit(tmp_TT_crops$crop[tmp_TT_crops$species == "cereal"]))
#learning_rate = 0.01
#epochs = 10
#test_sample_size = 10 # nb of randomly selected obs per iteration
#tolerance = 1e-5 # relative convergence tolerance
# test if new parameters reduce MSE; else keep unchanged parameters
MSE_old <- BVIAS_loss_function_weight(input, id_cols,var_param,tmp_var_weight,crop_name_cereals)
test_sample_size = 100
tmp_base_cost <- BVIAS_loss_function_weight(input,id_cols,var_param,var_weight,crop_name_cereals)
tmp_var_weight = var_weight
var_weight_optim = foreach(epoch = 1:epochs,.combine = 'rbind',.options.RNG=1234,
.packages=c("dplyr","groundhog","foreach","doParallel")) %dorng% {
#for (epoch in 1:epochs) {
source("d:/users/srhuet/documents/BiodivLabel/R/BVIAS_functions.R")
# Randomly sample some data point
index <- sample(1:nrow(input), test_sample_size)
input_i <- input[index,]
# Compute the gradient of the loss function
## the gradient is obtained by calculating the partial derivative of the loss function
tmp_new_cost <-  BVIAS_loss_function_weight(input_i, id_cols,var_param,var_weight,crop_name_cereals)
## WIP I think I should compute one gradient per parameter based on the partial derivative of the loss function according to this parameter
tmp_gradient <- -2/test_sample_size * sum(tmp_new_cost$distance_table$litt_values - tmp_new_cost$distance_table$BVAS)
# Update model parameters
tmpl_var_weight = foreach(r = seq_len(nrow(tmp_var_weight)), .combine = 'rbind',.options.RNG=1234,
.packages=c("dplyr","groundhog","foreach","doParallel")) %dorng% {
source("d:/users/srhuet/documents/BiodivLabel/R/BVIAS_functions.R")
tmpl_var_weight <- tmp_var_weight %>%
mutate(weight = weight - learning_rate * tmp_gradient)
tmpl_var_weight
}
# test if new parameters reduce MSE; else keep unchanged parameters
MSE_old <- BVIAS_loss_function_weight(input, id_cols,var_param,tmp_var_weight,crop_name_cereals)
MSE_new <- BVIAS_loss_function_weight(input, , id_cols,var_param,tmpl_var_weight,crop_name_cereals)
if (!is.na(MSE_new$MSE) & MSE_new$MSE < MSE_old$MSE) {
tmp_var_weight <- tmpl_var_weight
var_weight_optim <- tmp_var_weight %>% mutate(MSE = MSE_new$MSE)
} else {
#var_param_optim <- tmp_var_param %>% mutate(MSE = MSE_old$MSE)
print(paste0("Model converged at epoch ",epoch))
#break
}
var_weight_optim
}
# Randomly sample some data point
index <- sample(1:nrow(input), test_sample_size)
input_i <- input[index,]
# Compute the gradient of the loss function
## the gradient is obtained by calculating the partial derivative of the loss function
tmp_new_cost <-  BVIAS_loss_function_weight(input_i, id_cols,var_param,var_weight,crop_name_cereals)
## WIP I think I should compute one gradient per parameter based on the partial derivative of the loss function according to this parameter
tmp_gradient <- -2/test_sample_size * sum(tmp_new_cost$distance_table$litt_values - tmp_new_cost$distance_table$BVAS)
# Update model parameters
tmpl_var_weight = foreach(r = seq_len(nrow(tmp_var_weight)), .combine = 'rbind',.options.RNG=1234,
.packages=c("dplyr","groundhog","foreach","doParallel")) %dorng% {
source("d:/users/srhuet/documents/BiodivLabel/R/BVIAS_functions.R")
tmpl_var_weight <- tmp_var_weight %>%
mutate(weight = weight - learning_rate * tmp_gradient)
tmpl_var_weight
}
# test if new parameters reduce MSE; else keep unchanged parameters
MSE_old <- BVIAS_loss_function_weight(input, id_cols,var_param,tmp_var_weight,crop_name_cereals)
MSE_new <- BVIAS_loss_function_weight(input, , id_cols,var_param,tmpl_var_weight,crop_name_cereals)
MSE_new <- BVIAS_loss_function_weight(input, id_cols,var_param,tmpl_var_weight,crop_name_cereals)
# test if new parameters reduce MSE; else keep unchanged parameters
MSE_old <- BVIAS_loss_function_weight(input, id_cols,var_param,tmp_var_weight,crop_name_cereals)
MSE_new <- BVIAS_loss_function_weight(input, id_cols,var_param,tmpl_var_weight,crop_name_cereals)
View(tmpl_var_weight)
r=1
# Randomly sample some data point
index <- sample(1:nrow(input), test_sample_size)
input_i <- input[index,]
# Compute the gradient of the loss function
## the gradient is obtained by calculating the partial derivative of the loss function
tmp_new_cost <-  BVIAS_loss_function_weight(input_i, id_cols,var_param,var_weight,crop_name_cereals)
## WIP I think I should compute one gradient per parameter based on the partial derivative of the loss function according to this parameter
tmp_gradient <- -2/test_sample_size * sum(tmp_new_cost$distance_table$litt_values - tmp_new_cost$distance_table$BVAS)
# Update model parameters
tmpl_var_weight <- tmp_var_weight %>%
mutate(weight = weight - learning_rate * tmp_gradient)
# test if new parameters reduce MSE; else keep unchanged parameters
MSE_old <- BVIAS_loss_function_weight(input, id_cols,var_param,tmp_var_weight,crop_name_cereals)
MSE_new <- BVIAS_loss_function_weight(input, id_cols,var_param,tmpl_var_weight,crop_name_cereals)
if (!is.na(MSE_new$MSE) & MSE_new$MSE < MSE_old$MSE) {
tmp_var_weight <- tmpl_var_weight
var_weight_optim <- tmp_var_weight %>% mutate(MSE = MSE_new$MSE)
} else {
#var_param_optim <- tmp_var_param %>% mutate(MSE = MSE_old$MSE)
print(paste0("Model converged at epoch ",epoch))
#break
}
learning_rate * tmp_gradient
MSE_new$MSE < MSE_old$MSE
